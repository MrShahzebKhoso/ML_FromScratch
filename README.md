
# LightML

LightML is a **from-scratch Machine Learning library** built using Python and NumPy.
It includes core components used in ML: activation functions, optimizers, probability distributions, metrics, distances, and simple classifiers.  
This project is part of a continuous effort to build an educational, transparent ML toolkit without relying on highâ€‘level frameworks.
Also includes examples (pytorch based) of training, evaluations of algorithms and their applications. Including classification, detection, generation etc.
---

## ğŸ“¦ Project Structure

```
LightML/
â”‚
â”œâ”€â”€ lightml/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ activations/
â”‚   â”œâ”€â”€ optimizers/
â”‚   â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ distance/
â”‚   â”œâ”€â”€ probability/
â”‚   â”œâ”€â”€ statistics/
â”‚   â””â”€â”€ classifiers/
â”œâ”€â”€ setup.py
â”œâ”€â”€ setup.cfg
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Features

### **ğŸ“ Activation Functions**
- ReLU  
- Leaky ReLU  
- Sigmoid  
- Tanh  
- Softmax  
- GELU  

### **ğŸ§® Optimizers**
- Gradient Descent  
- Momentum / NAG  
- RMSProp  
- Adam  
- AdamW  
- Nadam  
- AdaGrad  
- AdaDelta  

### **ğŸ“Š Metrics**
- F1 Score  
- RÂ² Score    

### **ğŸ“ Distance Measures**
- Euclidean  
- Manhattan  
- Cosine Similarity  

### **ğŸ² Probability Distributions**
- Bernoulli  
- Binomial  
- Geometric  

### **ğŸ“ˆ Statistics**
- Mean, Median, Mode  
- Variance & Standard Deviation  
- Percentiles  
- Min-max & Z-score Normalization  
- Entropy  

### **ğŸŸ¢ Simple Classifiers**
- Majority Classifier (baseline)

---

## ğŸ“¥ Installation

Clone the repository:

```bash
git clone https://github.com/MrShahzebKhoso/ML_FromScratch.git
cd ML_FromScratch
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Install package locally:

```bash
pip install .
```

---

## ğŸ§ª Usage Examples

### **Example 1 â€” Using an Activation Function**
```python
import numpy as np
from lightml.activations.sigmoid import sigmoid
from lightml.activations.relu import relu

x = np.array([-2, -1, 0, 1, 2])
x_sigmoid = sigmoid(x)
x_relu = relu(x)
print(f"Sigmoid: {x_sigmoid}")
print(f"ReLU:{x_relu}")

```



### **Example 2 â€” F1 Micro**
```python
import numpy as np
from lightml.metrics.f1_micro import f1_micro

y_true = np.array([1,0,1,1])
y_pred = np.array([1,1,1,0])
f1_score = f1_micro(y_true, y_pred)
print(f"Micro F1: {f1_score}")

```

---

## ğŸ“Œ Roadmap (Upcoming)
- Neural network layers (Dense, Dropout)  
- Model training loop  
- Regression & classification models  
- Dataset utilities  
- Documentation website  

---

## ğŸ¤ Contributing
Contributions are welcome!  
Submit issues, feature requests, or pull requests.

---

## ğŸ“œ License
MIT License

---

## â­ Support
If you like this project, please â­ star the repo!
